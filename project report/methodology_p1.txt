# 3. Methodology

## 3.1 Environment

### 3.1.1 Python Version
Our project was implemented using Python 3.9.7, which provided compatibility with all the required machine learning libraries while maintaining stability. This version offers key features necessary for natural language processing tasks, including improved type hinting and dictionary merging operations.

### 3.1.2 Dependencies and Packages
The following dependencies were essential for our implementation:

- **TensorFlow 2.8.0**: Used for implementing the model architecture, training loops, and embedding generation
- **Sentence-Transformers 2.2.2**: Required for loading and utilizing the pre-trained ST5-base model
- **Numpy 1.22.3**: Used for efficient array operations and embedding manipulations
- **Pandas 1.4.2**: Used for data management and preprocessing
- **Scikit-learn 1.0.2**: Used for evaluation metrics, cross-validation splits, and similarity computations
- **NLTK 3.7**: Used for text processing and augmentation techniques
- **Matplotlib 3.5.1** and **Seaborn 0.11.2**: Used for visualization of results and data distributions

The complete set of dependencies can be found in the `requirements.txt` file in our project repository. To set up the environment, we created a virtual environment using:

```bash
python -m venv 598_env
source 598_env/bin/activate
pip install -r requirements.txt
```

## 3.2 Data

### 3.2.1 Data Download Instructions

Two primary datasets were required for this project:

1. **LOINC Database**:
   - Available from the LOINC organization at https://loinc.org/downloads/
   - Requires free registration to access
   - We used version 2.72 (as specified in the paper)
   - The downloaded file contains multiple CSV files, of which we used the primary `LOINC.csv` file

2. **MIMIC-III Database**:
   - Available from PhysioNet at https://physionet.org/content/mimiciii/1.4/
   - Requires completion of a data usage agreement and CITI training
   - We utilized only the `D_LABITEMS.csv` file from this dataset, which contains laboratory test definitions

After downloading, the data files were placed in a `data/` directory within our project structure:
```
data/
├── LOINC.csv               # LOINC database file
└── D_LABITEMS.csv          # MIMIC-III laboratory items definitions
```

### 3.2.2 Data Description and Visualization

#### LOINC Dataset

The LOINC database (version 2.72) contains comprehensive information about laboratory and clinical observations structured along six dimensions (component, property, time, system, scale, and method). The full database includes over 80,000 distinct LOINC codes, but for computational efficiency, we randomly sampled 10% (approximately 7,800 codes) while maintaining the distribution of different LOINC types.

**Key fields used from the LOINC database:**

| Field | Description | Example |
|-------|-------------|---------|
| LOINC_NUM | Unique LOINC identifier | "2160-0" |
| COMPONENT | What is measured | "Creatinine" |
| SYSTEM | Where the measurement is made | "Serum or Plasma" |
| LONG_COMMON_NAME | Full descriptive name | "Creatinine [Mass/volume] in Serum or Plasma" |
| SHORTNAME | Abbreviated version | "Creat SerPl-mCnc" |
| DISPLAY_NAME | Display version | "Creatinine" |
| RELATEDNAMES2 | Related terms and synonyms | "Creat; Serum creatinine; Plasma creatinine" |
| SCALE_TYP | Scale type (Qn, Ql, Ord, etc.) | "Qn" |

**Distribution of LOINC Scale Types in our sample:**
- Quantitative (Qn): 51.8%
- Qualitative (Ql): 25.3% 
- Ordinal (Ord): 14.2%
- Nominal (Nom): 8.1%
- Count (Cnt): 0.6%

**Distribution of LOINC Component Types:**
The top 10 components by frequency were:
1. Glucose: 2.1%
2. Erythrocytes: 1.9%
3. Leukocytes: 1.8% 
4. Cholesterol: 1.5%
5. Creatinine: 1.3%
6. Sodium: 1.2%
7. Potassium: 1.1%
8. Hemoglobin: 0.9%
9. Calcium: 0.9%
10. Platelets: 0.8%

#### MIMIC-III D_LABITEMS Dataset

The MIMIC-III D_LABITEMS table contains definitions for laboratory items used in the MIMIC-III database. From this dataset, we extracted 579 source-target pairs with 571 unique LOINC targets, matching the numbers reported in the paper.

**Key fields used from D_LABITEMS:**

| Field | Description | Example |
|-------|-------------|---------|
| ITEMID | Unique identifier for the lab test | 50971 |
| LABEL | Text description of the lab test | "Creatinine" |
| FLUID | Specimen type | "Blood" |
| LOINC_CODE | Mapped LOINC code (if available) | "2160-0" |

**Examples of source-target pairs:**

| Source Text (LABEL + FLUID) | Target LOINC | LOINC Description |
|-----------------------------|--------------|-------------------|
| "creatinine blood" | 2160-0 | "Creatinine [Mass/volume] in Serum or Plasma" |
| "glucose blood" | 2345-7 | "Glucose [Mass/volume] in Serum or Plasma" |
| "hemoglobin blood" | 718-7 | "Hemoglobin [Mass/volume] in Blood" |

**Distribution analysis:**
- 579 source-target pairs contained 571 unique LOINC targets
- The majority (98.6%) of LOINC codes appeared in only one source-target pair
- Only 8 LOINC codes had multiple source representations

### 3.2.3 Data Preprocessing

Our data preprocessing pipeline followed the methodology described in the paper, with the following key steps:

1. **LOINC Processing:**
```python
def process_loinc_data(loinc_file, output_sample_size=0.1, random_seed=42):
    """
    Process the LOINC database file and sample a subset for training.
    
    Args:
        loinc_file: Path to the LOINC.csv file
        output_sample_size: Fraction of LOINC codes to sample (0.1 = 10%)
        random_seed: Random seed for reproducibility
        
    Returns:
        Pandas DataFrame with processed LOINC data
    """
    # Read LOINC file
    columns_to_use = [
        'LOINC_NUM', 'COMPONENT', 'PROPERTY', 'TIME_ASPCT', 
        'SYSTEM', 'SCALE_TYP', 'METHOD_TYP', 'CLASS', 
        'LONG_COMMON_NAME', 'SHORTNAME', 'DISPLAY_NAME', 'RELATEDNAMES2'
    ]
    
    loinc_df = pd.read_csv(loinc_file, usecols=columns_to_use, low_memory=False)
    
    # Filter to laboratory and clinical observations only
    lab_classes = ['CHEM', 'HEM/BC', 'MICRO', 'SERO', 'PANEL.CHEM', 'PANEL.HEM/BC']
    loinc_df = loinc_df[loinc_df['CLASS'].isin(lab_classes)]
    
    # Sample a random subset to make computation feasible
    unique_loinc_codes = loinc_df['LOINC_NUM'].unique()
    sampled_codes = np.random.RandomState(random_seed).choice(
        unique_loinc_codes,
        size=int(len(unique_loinc_codes) * output_sample_size),
        replace=False
    )
    
    loinc_df = loinc_df[loinc_df['LOINC_NUM'].isin(sampled_codes)]
    
    # Convert text fields to lowercase
    text_columns = ['LONG_COMMON_NAME', 'SHORTNAME', 'DISPLAY_NAME', 'RELATEDNAMES2']
    for col in text_columns:
        loinc_df[col] = loinc_df[col].str.lower().fillna('')
    
    return loinc_df
```

2. **MIMIC-III Processing:**
```python
def process_mimic_data(d_labitems_file):
    """
    Process the MIMIC-III D_LABITEMS.csv file to create source-target pairs.
    
    Args:
        d_labitems_file: Path to the D_LABITEMS.csv file
        
    Returns:
        Pandas DataFrame with source-target pairs
    """
    # Read D_LABITEMS file
    labitems_df = pd.read_csv(d_labitems_file)
    
    # Filter rows where LOINC_CODE is not missing
    labitems_df = labitems_df.dropna(subset=['LOINC_CODE'])
    
    # Create source text by concatenating LABEL and FLUID
    labitems_df['source_text'] = labitems_df.apply(
        lambda row: (str(row['LABEL']) + ' ' + str(row['FLUID'])).lower(),
        axis=1
    )
    
    # Rename LOINC_CODE to target_loinc
    labitems_df = labitems_df.rename(columns={'LOINC_CODE': 'target_loinc'})
    
    # Select relevant columns
    mimic_pairs_df = labitems_df[['ITEMID', 'source_text', 'target_loinc']]
    
    return mimic_pairs_df
```

3. **Data Augmentation:**
```python
def augment_text(text, augmentation_factor=5):
    """
    Apply data augmentation techniques to a text string.
    
    Args:
        text: Text string to augment
        augmentation_factor: Number of augmented versions to generate
        
    Returns:
        List of augmented text strings
    """
    augmented_texts = [text]  # Original text
    
    for _ in range(augmentation_factor):
        # Randomly choose an augmentation technique
        technique = np.random.choice([
            'character_deletion',
            'word_swapping',
            'word_insertion',
            'acronym_substitution'
        ])
        
        if technique == 'character_deletion':
            # Randomly delete characters
            chars = list(text)
            n_delete = max(1, int(len(chars) * 0.1))  # Delete up to 10% of characters
            delete_indices = np.random.choice(len(chars), size=n_delete, replace=False)
            new_text = ''.join([c for i, c in enumerate(chars) if i not in delete_indices])
            
        elif technique == 'word_swapping':
            # Randomly swap words
            words = text.split()
            if len(words) > 1:
                idx1, idx2 = np.random.choice(len(words), size=2, replace=False)
                words[idx1], words[idx2] = words[idx2], words[idx1]
                new_text = ' '.join(words)
            else:
                new_text = text
                
        elif technique == 'word_insertion':
            # Insert a related word (simplified implementation)
            words = text.split()
            related_words = ['lab', 'test', 'result', 'panel', 'count', 'level']
            insert_word = np.random.choice(related_words)
            insert_pos = np.random.randint(0, len(words) + 1)
            words.insert(insert_pos, insert_word)
            new_text = ' '.join(words)
            
        elif technique == 'acronym_substitution':
            # Replace words with acronyms (simplified implementation)
            common_replacements = {
                'blood': 'bld',
                'urine': 'ur',
                'serum': 'ser',
                'plasma': 'plas',
                'glucose': 'gluc',
                'creatinine': 'creat'
            }
            new_text = text
            for word, acronym in common_replacements.items():
                if word in new_text:
                    new_text = new_text.replace(word, acronym)
        
        if new_text and new_text != text:
            augmented_texts.append(new_text)
    
    return augmented_texts
```

4. **Training Data Preparation:**
```python
def prepare_training_data(loinc_df, mimic_pairs_df):
    """
    Prepare training data for both Stage 1 and Stage 2.
    
    Args:
        loinc_df: Processed LOINC DataFrame
        mimic_pairs_df: Processed MIMIC-III pairs DataFrame
        
    Returns:
        stage1_data: Dictionary with data for Stage 1 training
        stage2_data: Dictionary with data for Stage 2 training
    """
    # Prepare Stage 1 data (target-only)
    stage1_triplets = []
    loinc_targets = {}
    
    # Group by LOINC_NUM
    for loinc_num, group in loinc_df.groupby('LOINC_NUM'):
        # Get text variants for each LOINC code
        variants = []
        for _, row in group.iterrows():
            for field in ['LONG_COMMON_NAME', 'SHORTNAME', 'DISPLAY_NAME']:
                if pd.notna(row[field]) and row[field]:
                    variants.append(row[field])
            
            # Add related names if available
            if pd.notna(row['RELATEDNAMES2']) and row['RELATEDNAMES2']:
                related_names = [name.strip() for name in row['RELATEDNAMES2'].split(';')]
                variants.extend(related_names)
        
        # Store unique variants
        variants = list(set([v for v in variants if v]))
        if variants:
            loinc_targets[loinc_num] = variants
    
    # Generate triplets for Stage 1
    all_loinc_nums = list(loinc_targets.keys())
    for anchor_loinc in all_loinc_nums:
        anchor_variants = loinc_targets[anchor_loinc]
        if len(anchor_variants) < 2:
            continue
            
        # For each anchor variant
        for anchor_text in anchor_variants:
            # Select a different variant as positive
            positive_candidates = [v for v in anchor_variants if v != anchor_text]
            if not positive_candidates:
                continue
            positive_text = np.random.choice(positive_candidates)
            
            # Select a different LOINC code as negative
            negative_candidates = [loinc for loinc in all_loinc_nums 
                                 if loinc != anchor_loinc]
            negative_loinc = np.random.choice(negative_candidates)
            negative_text = np.random.choice(loinc_targets[negative_loinc])
            
            # Apply augmentation
            for aug_anchor in augment_text(anchor_text):
                for aug_positive in augment_text(positive_text):
                    for aug_negative in augment_text(negative_text):
                        stage1_triplets.append({
                            'anchor': aug_anchor,
                            'positive': aug_positive,
                            'negative': aug_negative
                        })
    
    # Prepare Stage 2 data (source-target pairs)
    # Create a mapping from LOINC code to its text representations
    loinc_text_map = {}
    for loinc_num, variants in loinc_targets.items():
        loinc_text_map[loinc_num] = variants[0] if variants else ""
    
    # Add target text to mimic_pairs_df
    mimic_pairs_df['target_text'] = mimic_pairs_df['target_loinc'].map(loinc_text_map)
    
    # Apply augmentation to source and target texts
    stage2_pairs = []
    for _, row in mimic_pairs_df.iterrows():
        source_text = row['source_text']
        target_text = row['target_text']
        target_loinc = row['target_loinc']
        
        if not target_text:  # Skip if no target text found
            continue
            
        for aug_source in augment_text(source_text):
            for aug_target in augment_text(target_text):
                stage2_pairs.append({
                    'source_text': aug_source,
                    'target_text': aug_target,
                    'target_loinc': target_loinc
                })
    
    return {
        'triplets': stage1_triplets,
        'loinc_targets': loinc_targets
    }, {
        'pairs': stage2_pairs,
        'mimic_df': mimic_pairs_df
    }
```

These preprocessing steps ensured that our data were properly prepared for the two-stage training approach described in the paper.

## 3.3 Model

### 3.3.1 Original Paper Repository

The original paper did not provide a public code repository. Our implementation is based entirely on the methodology described in the paper.

### 3.3.2 Model Description

Our model architecture closely follows the design described in the paper, consisting of a pre-trained Sentence-T5 (ST5-base) encoder followed by a dense projection layer. The key components of the model are:

#### Encoder Architecture

The core of our model uses the pre-trained Sentence-T5 (ST5-base) encoder, which is a variant of the T5 transformer model optimized for sentence encoding tasks. The ST5-base model consists of:
- 12 transformer layers
- 768-dimensional hidden states
- 12 attention heads
- Approximately 110 million parameters

We kept the ST5-base encoder weights frozen during training, as specified in the paper, to prevent overfitting given the limited training data.

#### Projection Layer

The 768-dimensional embeddings from the ST5 encoder are projected down to 128 dimensions using a fully-connected layer. This projection layer is the only component that is trained during the fine-tuning process. The projected embeddings are then L2-normalized to ensure that similarity measurements using cosine distance are properly scaled.

The model architecture is illustrated below:

```
Input Text → ST5-base Encoder (frozen) → 768-dim Embedding → Dense Layer → 128-dim Embedding → L2 Normalization → Final Embedding
```

#### Triplet Loss Function

For training, we implemented the triplet loss function as described in the paper:

$$L = \max(0, D_{cos}^2(f(x_a), f(x_p)) - D_{cos}^2(f(x_a), f(x_n)) + \alpha)$$

Where:
- $f(x)$ is the final 128-dimensional L2-normalized embedding for input text $x$
- $x_a$ is the anchor sample
- $x_p$ is the positive sample (same class as anchor)
- $x_n$ is the negative sample (different class from anchor)
- $D_{cos}$ is the cosine distance, defined as $D_{cos}(a, b) = 1 - \frac{a \cdot b}{||a|| \cdot ||b||}$
- $\alpha$ is the margin hyperparameter (set to 0.8 in the paper)

The loss function encourages the model to learn embeddings where samples from the same class (anchor and positive) are closer together than samples from different classes (anchor and negative), with a margin of $\alpha$.

#### Model Implementation

```python
class LOINCEmbeddingModel(tf.keras.Model):
    def __init__(self, encoder_model_path="sentence-transformers/sentence-t5-base", 
                 projection_dim=128):
        super(LOINCEmbeddingModel, self).__init__()
        
        # Load pre-trained ST5 encoder
        self.encoder = TFAutoModel.from_pretrained(encoder_model_path)
        self.tokenizer = AutoTokenizer.from_pretrained(encoder_model_path)
        
        # Freeze the encoder weights
        self.encoder.trainable = False
        
        # Projection layer (768 → 128 dimensions)
        self.projection = tf.keras.layers.Dense(projection_dim)
        
    def call(self, inputs, training=False):
        # Tokenize inputs
        tokenized_inputs = self.tokenizer(
            inputs, 
            padding=True, 
            truncation=True, 
            return_tensors="tf"
        )
        
        # Get encoder outputs
        encoder_outputs = self.encoder(
            input_ids=tokenized_inputs["input_ids"],
            attention_mask=tokenized_inputs["attention_mask"]
        )
        
        # Get sentence embeddings from last hidden state
        sentence_embeddings = encoder_outputs.last_hidden_state[:, 0, :]  # CLS token
        
        # Project to lower dimension
        projected_embeddings = self.projection(sentence_embeddings)
        
        # L2 normalize
        normalized_embeddings = tf.nn.l2_normalize(projected_embeddings, axis=1)
        
        return normalized_embeddings
    
    def compute_embeddings(self, texts, batch_size=32):
        """Compute embeddings for a list of texts"""
        all_embeddings = []
        
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i+batch_size]
            embeddings = self(batch_texts)
            all_embeddings.append(embeddings)
            
        return tf.concat(all_embeddings, axis=0)
```

#### Triplet Loss Implementation

```python
def triplet_loss(anchor, positive, negative, margin=0.8):
    """
    Compute triplet loss based on cosine distance.
    
    Args:
        anchor: Tensor of anchor embeddings (batch_size, embedding_dim)
        positive: Tensor of positive embeddings (batch_size, embedding_dim)
        negative: Tensor of negative embeddings (batch_size, embedding_dim)
        margin: Margin hyperparameter (alpha)
        
    Returns:
        Triplet loss value
    """
    # Compute cosine distances
    cos_pos = tf.reduce_sum(anchor * positive, axis=1)  # Dot product of normalized vectors = cosine similarity
    cos_neg = tf.reduce_sum(anchor * negative, axis=1)
    
    # Convert cosine similarity to cosine distance: d_cos = 1 - cos_sim
    d_pos = 1.0 - cos_pos
    d_neg = 1.0 - cos_neg
    
    # Square the distances as in the paper
    d_pos_squared = tf.square(d_pos)
    d_neg_squared = tf.square(d_neg)
    
    # Compute triplet loss
    loss = tf.maximum(0.0, d_pos_squared - d_neg_squared + margin)
    
    # Average over the batch
    return tf.reduce_mean(loss)
```

### 3.3.3 Model Inputs and Outputs

**Inputs**:
- Raw text strings, such as:
  - Source lab test descriptions (e.g., "creatinine blood")
  - Target LOINC descriptions (e.g., "Creatinine [Mass/volume] in Serum or Plasma")

**Processing Steps**:
1. Text tokenization using the ST5 tokenizer
2. Encoding through the frozen ST5-base encoder
3. Projection to 128 dimensions via the fully-connected layer
4. L2 normalization of the projected embeddings

**Outputs**:
- 128-dimensional L2-normalized embeddings that capture the semantic meaning of the input text
- These embeddings are used to compute cosine similarity during inference to find the most relevant LOINC codes for a given source text

### 3.3.4 Pretrained Model

Our implementation uses the pre-trained Sentence-T5 (ST5-base) model as the encoder backbone. This model was developed by Google Research and is available through the Hugging Face model hub at `sentence-transformers/sentence-t5-base`.

ST5-base was chosen because, as noted in the paper, it achieves state-of-the-art performance on sentence transfer tasks and outperforms other models like Sentence-BERT/RoBERTa across multiple semantic textual similarity tasks. The model was pre-trained using a contrastive approach similar to the one we employ for fine-tuning, making it well-suited for our task.

Our two-stage fine-tuning approach builds upon this pre-trained model, training only the projection layer while keeping the ST5 backbone frozen to prevent overfitting given our limited training data. 